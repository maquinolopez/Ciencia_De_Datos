#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Oct 22 17:21:54 2025

@author: maquinol
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression, make_moons
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree
from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score
import seaborn as sns

# Configuraci√≥n
plt.rcParams['font.size'] = 12
np.random.seed(42)

print("=" * 80)
print("TUTORIAL: √ÅRBOLES DE DECISI√ìN, BAGGING Y RANDOM FOREST CON SCIKIT-LEARN")
print("=" * 80)

# =============================================================================
# PARTE 1: CLASIFICACI√ìN
# =============================================================================
print("\n" + "=" * 60)
print("PARTE 1: PROBLEMA DE CLASIFICACI√ìN")
print("=" * 60)

# 1) Datos
print("\n1. Generando datos de clasificaci√≥n...")
X_clf, y_clf = make_classification(
    n_samples=1000,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=42
)
X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(
    X_clf, y_clf, test_size=0.3, random_state=42
)

print(f"   Forma de los datos: {X_clf.shape}")
print(f"   Clases: {np.unique(y_clf)}")
print(f"   Distribuci√≥n: {np.bincount(y_clf)}")

# 2) Visual: datos
def plot_decision_boundary(clf, X, y, ax, title):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7, edgecolors='k')
    ax.set_xlabel('Caracter√≠stica 1')
    ax.set_ylabel('Caracter√≠stica 2')
    ax.set_title(title)

plt.figure(figsize=(20, 5))
ax1 = plt.subplot(1, 4, 1)
ax1.scatter(X_clf[:, 0], X_clf[:, 1], c=y_clf, cmap='viridis', alpha=0.7, edgecolors='k')
ax1.set_xlabel('Caracter√≠stica 1')
ax1.set_ylabel('Caracter√≠stica 2')
ax1.set_title('Datos de Clasificaci√≥n\n(2 clases, 2 caracter√≠sticas)')
plt.colorbar(ax1.collections[0], ax=ax1, label='Clase')

# =============================================================================
# MODELOS DE CLASIFICACI√ìN
# =============================================================================
print("\n2. Entrenando modelos de clasificaci√≥n...")

# √Årbol de Decisi√≥n (base learner)
dt_clf = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_clf.fit(X_clf_train, y_clf_train)
y_dt_pred = dt_clf.predict(X_clf_test)
accuracy_dt = accuracy_score(y_clf_test, y_dt_pred)

bag_clf = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=5),  # <‚îÄ‚îÄ cambio aqu√≠
    n_estimators=100,
    max_samples=0.8,
    bootstrap=True,
    oob_score=True,
    random_state=42
)
bag_clf.fit(X_clf_train, y_clf_train)
y_bag_pred = bag_clf.predict(X_clf_test)
accuracy_bag = accuracy_score(y_clf_test, y_bag_pred)

# Random Forest
rf_clf = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42
)
rf_clf.fit(X_clf_train, y_clf_train)
y_rf_pred = rf_clf.predict(X_clf_test)
accuracy_rf = accuracy_score(y_clf_test, y_rf_pred)

print(f"   ‚úì √Årbol de Decisi√≥n:  acc = {accuracy_dt:.4f}")
print(f"   ‚úì Bagging (√°rboles):  acc = {accuracy_bag:.4f} | OOB = {bag_clf.oob_score_:.4f}")
print(f"   ‚úì Random Forest:      acc = {accuracy_rf:.4f}")

# 3) Visual: fronteras de decisi√≥n (√Årbol, Bagging, RF)
ax2 = plt.subplot(1, 4, 2)
plot_decision_boundary(dt_clf, X_clf_test, y_clf_test, ax2,
                       f'√Årbol de Decisi√≥n\nAccuracy: {accuracy_dt:.3f}')

ax3 = plt.subplot(1, 4, 3)
plot_decision_boundary(bag_clf, X_clf_test, y_clf_test, ax3,
                       f'Bagging (50 √°rboles)\nAcc: {accuracy_bag:.3f} | OOB: {bag_clf.oob_score_:.3f}')

ax4 = plt.subplot(1, 4, 4)
plot_decision_boundary(rf_clf, X_clf_test, y_clf_test, ax4,
                       f'Random Forest (50)\nAccuracy: {accuracy_rf:.3f}')

plt.tight_layout()
plt.savefig('clasificacion_comparacion_con_bagging.png', dpi=150, bbox_inches='tight')
plt.show()

# =============================================================================
# COMPARACI√ìN DETALLADA CLASIFICACI√ìN
# =============================================================================
print("\n3. Comparaci√≥n detallada - Clasificaci√≥n:")

# Validaci√≥n cruzada
models_clf = {
    '√Årbol Decisi√≥n': dt_clf,
    'Bagging': bag_clf,
    'Random Forest': rf_clf
}

cv_scores = {}
for name, model in models_clf.items():
    scores = cross_val_score(model, X_clf, y_clf, cv=5, scoring='accuracy')
    cv_scores[name] = scores
    print(f"   {name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")

# Gr√°fica de comparaci√≥n
plt.figure(figsize=(10, 6))
boxes = plt.boxplot(cv_scores.values(), labels=cv_scores.keys(), patch_artist=True)
colors = ['lightblue', 'lightgreen', 'lightcoral']
for patch, color in zip(boxes['boxes'], colors):
    patch.set_facecolor(color)

plt.ylabel('Accuracy')
plt.title('Comparaci√≥n de Modelos - Validaci√≥n Cruzada (5 folds)')
plt.grid(True, alpha=0.3)
plt.savefig('clasificacion_cv_comparacion.png', dpi=150, bbox_inches='tight')
plt.show()

# =============================================================================
# PARTE 2: REGRESI√ìN
# =============================================================================
print("\n" + "=" * 60)
print("PARTE 2: PROBLEMA DE REGRESI√ìN")
print("=" * 60)

# Generar datos de regresi√≥n
print("\n1. Generando datos de regresi√≥n...")
X_reg, y_reg = make_regression(
    n_samples=500,
    n_features=1,
    noise=20,
    random_state=42
)

X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(
    X_reg, y_reg, test_size=0.3, random_state=42
)

print(f"   Forma de los datos: {X_reg.shape}")
print(f"   Rango de valores Y: [{y_reg.min():.2f}, {y_reg.max():.2f}]")

# =============================================================================
# MODELOS DE REGRESI√ìN
# =============================================================================
print("\n2. Entrenando modelos de regresi√≥n...")

# √Årbol de Regresi√≥n
dt_reg = DecisionTreeRegressor(max_depth=5, random_state=42)
dt_reg.fit(X_reg_train, y_reg_train)
y_dt_reg_pred = dt_reg.predict(X_reg_test)
mse_dt = mean_squared_error(y_reg_test, y_dt_reg_pred)
r2_dt = r2_score(y_reg_test, y_dt_reg_pred)

# Bagging Regressor
bag_reg = BaggingRegressor(
    DecisionTreeRegressor(max_depth=5),
    n_estimators=100,
    max_samples=0.8,
    random_state=42
)
bag_reg.fit(X_reg_train, y_reg_train)
y_bag_reg_pred = bag_reg.predict(X_reg_test)
mse_bag = mean_squared_error(y_reg_test, y_bag_reg_pred)
r2_bag = r2_score(y_reg_test, y_bag_reg_pred)

# Random Forest Regressor
rf_reg = RandomForestRegressor(
    n_estimators=100,
    max_depth=5,
    random_state=42
)
rf_reg.fit(X_reg_train, y_reg_train)
y_rf_reg_pred = rf_reg.predict(X_reg_test)
mse_rf = mean_squared_error(y_reg_test, y_rf_reg_pred)
r2_rf = r2_score(y_reg_test, y_rf_reg_pred)

print(f"   ‚úì √Årbol de Regresi√≥n - MSE: {mse_dt:.2f}, R¬≤: {r2_dt:.4f}")
print(f"   ‚úì Bagging Regressor - MSE: {mse_bag:.2f}, R¬≤: {r2_bag:.4f}")
print(f"   ‚úì Random Forest - MSE: {mse_rf:.2f}, R¬≤: {r2_rf:.4f}")

# Visualizar resultados de regresi√≥n
plt.figure(figsize=(15, 5))

# Ordenar datos para plotting
sort_idx = np.argsort(X_reg_test.flatten())
X_sorted = X_reg_test[sort_idx]
y_true_sorted = y_reg_test[sort_idx]

plt.subplot(1, 3, 1)
plt.scatter(X_reg, y_reg, alpha=0.3, label='Datos')
plt.plot(X_sorted, y_dt_reg_pred[sort_idx], 'r-', linewidth=2, label='Predicci√≥n')
plt.xlabel('X')
plt.ylabel('y')
plt.title(f'√Årbol de Regresi√≥n\nMSE: {mse_dt:.2f}, R¬≤: {r2_dt:.4f}')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
plt.scatter(X_reg, y_reg, alpha=0.3, label='Datos')
plt.plot(X_sorted, y_bag_reg_pred[sort_idx], 'g-', linewidth=2, label='Predicci√≥n')
plt.xlabel('X')
plt.ylabel('y')
plt.title(f'Bagging Regressor\nMSE: {mse_bag:.2f}, R¬≤: {r2_bag:.4f}')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
plt.scatter(X_reg, y_reg, alpha=0.3, label='Datos')
plt.plot(X_sorted, y_rf_reg_pred[sort_idx], 'b-', linewidth=2, label='Predicci√≥n')
plt.xlabel('X')
plt.ylabel('y')
plt.title(f'Random Forest\nMSE: {mse_rf:.2f}, R¬≤: {r2_rf:.4f}')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('regresion_comparacion.png', dpi=150, bbox_inches='tight')
plt.show()

# =============================================================================
# COMPARACI√ìN DETALLADA REGRESI√ìN
# =============================================================================
print("\n3. Comparaci√≥n detallada - Regresi√≥n:")

models_reg = {
    '√Årbol Regresi√≥n': dt_reg,
    'Bagging': bag_reg,
    'Random Forest': rf_reg
}

cv_scores_reg = {}
for name, model in models_reg.items():
    scores = cross_val_score(model, X_reg, y_reg, cv=10, scoring='r2')
    cv_scores_reg[name] = scores
    print(f"   {name} - R¬≤: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")

# Gr√°fica de comparaci√≥n R¬≤
plt.figure(figsize=(10, 6))
boxes = plt.boxplot(cv_scores_reg.values(), labels=cv_scores_reg.keys(), patch_artist=True)
colors = ['lightblue', 'lightgreen', 'lightcoral']
for patch, color in zip(boxes['boxes'], colors):
    patch.set_facecolor(color)

plt.ylabel('R¬≤ Score')
plt.title('Comparaci√≥n de Modelos de Regresi√≥n - Validaci√≥n Cruzada (5 folds)')
plt.grid(True, alpha=0.3)
plt.savefig('regresion_cv_comparacion.png', dpi=150, bbox_inches='tight')
plt.show()

# =============================================================================
# PARTE 3: EJEMPLO PR√ÅCTICO - DATOS COMPLEJOS
# =============================================================================
print("\n" + "=" * 60)
print("PARTE 3: EJEMPLO PR√ÅCTICO - DATOS COMPLEJOS (Moons)")
print("=" * 60)

# Datos complejos no lineales
X_moons, y_moons = make_moons(n_samples=1000, noise=0.3, random_state=42)
X_m_train, X_m_test, y_m_train, y_m_test = train_test_split(
    X_moons, y_moons, test_size=0.3, random_state=42
)

print(f"Datos Moons - Forma: {X_moons.shape}")

# Comparaci√≥n en datos complejos
models_moons = {
    '√Årbol Simple': DecisionTreeClassifier(max_depth=3, random_state=42),
    '√Årbol Profundo': DecisionTreeClassifier(max_depth=20, random_state=42),
    'Bagging': BaggingClassifier(n_estimators=50, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42)
}

results_moons = {}
plt.figure(figsize=(15, 10))

for i, (name, model) in enumerate(models_moons.items(), 1):
    model.fit(X_m_train, y_m_train)
    y_pred = model.predict(X_m_test)
    accuracy = accuracy_score(y_m_test, y_pred)
    results_moons[name] = accuracy
    
    plt.subplot(2, 2, i)
    plot_decision_boundary(model, X_m_test, y_m_test, plt.gca(), 
                          f'{name}\nAccuracy: {accuracy:.3f}')

plt.tight_layout()
plt.savefig('moons_comparacion.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nResultados en datos Moons (no lineales):")
for name, acc in results_moons.items():
    print(f"   {name}: {acc:.4f}")

# =============================================================================
# PARTE 4: RESUMEN Y RECOMENDACIONES
# =============================================================================
print("\n" + "=" * 60)
print("PARTE 4: RESUMEN Y RECOMENDACIONES")
print("=" * 60)

print("""
RESUMEN DE MODELOS:

1. √ÅRBOL DE DECISI√ìN:
   - ‚úÖ Ventajas: F√°cil de interpretar, r√°pido de entrenar
   - ‚ùå Desventajas: Propenso a sobreajuste, alta varianza
   - üìä Uso: Datos peque√±os, interpretabilidad importante

2. BAGGING (Bootstrap Aggregating):
   - ‚úÖ Ventajas: Reduce varianza, m√°s estable que √°rbol simple
   - ‚ùå Desventajas: Menos interpretable, requiere m√°s c√≥mputo
   - üìä Uso: Cuando se quiere mejorar un modelo base estable

3. RANDOM FOREST:
   - ‚úÖ Ventajas: Reduce varianza y sobreajuste, robusto
   - ‚ùå Desventajas: Menos interpretable, hiperpar√°metros a ajustar
   - üìä Uso: Problemas generales, buen rendimiento out-of-the-box

PAR√ÅMETROS IMPORTANTES:

‚Ä¢ √Årbol de Decisi√≥n:
  - max_depth: Profundidad m√°xima (controla complejidad)
  - min_samples_split: M√≠nimo muestras para dividir nodo
  - min_samples_leaf: M√≠nimo muestras en hoja

‚Ä¢ Bagging:
  - n_estimators: N√∫mero de modelos base
  - max_samples: Fracci√≥n de muestras por modelo
  - base_estimator: Modelo base a usar

‚Ä¢ Random Forest:
  - n_estimators: N√∫mero de √°rboles
  - max_depth: Profundidad de √°rboles
  - max_features: Caracter√≠sticas por divisi√≥n
  - min_samples_split: M√≠nimo para dividir

BUENAS PR√ÅCTICAS:

1. Siempre usar validaci√≥n cruzada
2. Comenzar con Random Forest como baseline
3. Ajustar hiperpar√°metros con GridSearchCV
4. Considerar interpretabilidad vs rendimiento
5. Usar √Årbol simple para entender los datos
""")

# =============================================================================
# EJEMPLO DE OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS
# =============================================================================
print("\n" + "=" * 60)
print("EJEMPLO: OPTIMIZACI√ìN DE RANDOM FOREST")
print("=" * 60)

from sklearn.model_selection import GridSearchCV

# B√∫squeda de grilla simple
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_depth': [3, 5, 7, 15, None],
    'min_samples_split': [2, 5, 10,100]
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_clf_train, y_clf_train)

print(f"Mejores par√°metros: {grid_search.best_params_}")
print(f"Mejor score: {grid_search.best_score_:.4f}")

# Comparaci√≥n antes/despu√©s de optimizaci√≥n
rf_base = RandomForestClassifier(n_estimators=50, random_state=42)
rf_base.fit(X_clf_train, y_clf_train)
y_base_pred = rf_base.predict(X_clf_test)
accuracy_base = accuracy_score(y_clf_test, y_base_pred)

rf_optimized = grid_search.best_estimator_
y_opt_pred = rf_optimized.predict(X_clf_test)
accuracy_opt = accuracy_score(y_clf_test, y_opt_pred)

print(f"\nComparaci√≥n Random Forest:")
print(f"   Base (n_estimators=50): {accuracy_base:.4f}")
print(f"   Optimizado: {accuracy_opt:.4f}")
print(f"   Mejora: {(accuracy_opt - accuracy_base)*100:.2f}%")


# Mostrar feature importance
plt.figure(figsize=(10, 6))
feature_importance = rf_clf.feature_importances_
features = ['Feature 1', 'Feature 2']
plt.bar(features, feature_importance)
plt.title('Feature Importance - Random Forest')
plt.ylabel('Importancia')
plt.grid(True, alpha=0.3)
plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"Feature Importance del Random Forest:")
for feat, imp in zip(features, feature_importance):
    print(f"   {feat}: {imp:.4f}")